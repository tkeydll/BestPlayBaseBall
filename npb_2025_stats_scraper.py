#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NPB 2025å¹´åº¦çƒå›£åˆ¥å€‹äººæˆç¸¾å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚»ãƒªãƒ¼ã‚°ãƒ»ãƒ‘ãƒªãƒ¼ã‚°å…¨12çƒå›£ã®æ‰“æ’ƒãƒ»æŠ•æ‰‹ãƒ»å®ˆå‚™æˆç¸¾ã‚’CSVã§ä¿å­˜
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from urllib.parse import urljoin

# NPBã®çƒå›£ã‚³ãƒ¼ãƒ‰ã¨ãƒãƒ¼ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°
TEAMS = {
    # ã‚»ãƒ³ãƒˆãƒ©ãƒ«ãƒ»ãƒªãƒ¼ã‚°
    'g': 'èª­å£²ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„',
    't': 'é˜ªç¥ã‚¿ã‚¤ã‚¬ãƒ¼ã‚¹', 
    'db': 'æ¨ªæµœDeNAãƒ™ã‚¤ã‚¹ã‚¿ãƒ¼ã‚º',
    'c': 'åºƒå³¶æ±æ´‹ã‚«ãƒ¼ãƒ—',
    's': 'æ±äº¬ãƒ¤ã‚¯ãƒ«ãƒˆã‚¹ãƒ¯ãƒ­ãƒ¼ã‚º',
    'd': 'ä¸­æ—¥ãƒ‰ãƒ©ã‚´ãƒ³ã‚º',
    # ãƒ‘ã‚·ãƒ•ã‚£ãƒƒã‚¯ãƒ»ãƒªãƒ¼ã‚°
    'h': 'ç¦å²¡ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ãƒ›ãƒ¼ã‚¯ã‚¹',
    'f': 'åŒ—æµ·é“æ—¥æœ¬ãƒãƒ ãƒ•ã‚¡ã‚¤ã‚¿ãƒ¼ã‚º',
    'm': 'åƒè‘‰ãƒ­ãƒƒãƒ†ãƒãƒªãƒ¼ãƒ³ã‚º',
    'e': 'æ±åŒ—æ¥½å¤©ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚¤ãƒ¼ã‚°ãƒ«ã‚¹',
    'b': 'ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒ»ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚º',
    'l': 'åŸ¼ç‰è¥¿æ­¦ãƒ©ã‚¤ã‚ªãƒ³ã‚º'
}

# æˆç¸¾ã®ç¨®é¡
STATS_TYPES = {
    'bat': 'æ‰“æ’ƒæˆç¸¾',
    'pit': 'æŠ•æ‰‹æˆç¸¾', 
    'fld': 'å®ˆå‚™æˆç¸¾'
}

BASE_URL = 'https://npb.jp/bis/2025/stats/'

def get_page_content(url):
    """Webãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.encoding = 'utf-8'
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {url} - {e}")
        return None

def parse_stats_table(html_content, team_name, stats_type):
    """çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è§£æã—ã¦DataFrameã«å¤‰æ›"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æ¤œç´¢ï¼ˆ<tr>ã‚¿ã‚°ã‚’ç›´æ¥æ¢ã™ï¼‰
    rows = []
    all_trs = soup.find_all('tr')
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼å€™è£œã‚’æ¢ã™ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼‰
    headers = []
    header_found = False
    
    for tr in all_trs:
        cells = tr.find_all(['td', 'th'])
        if not cells:
            continue
            
        row_data = [cell.get_text(strip=True) for cell in cells]
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ç‰¹å®šï¼ˆé¸æ‰‹åãŒå«ã¾ã‚Œã¦ã„ãªã„è¡Œï¼‰
        if not header_found and len(row_data) > 5:
            # ãƒ˜ãƒƒãƒ€ãƒ¼å€™è£œã‚’è¨­å®š
            if not headers:
                # çµ±è¨ˆé …ç›®åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if any(item in str(row_data) for item in ['æ‰“ç‡', 'æœ¬å¡æ‰“', 'é˜²å¾¡ç‡', 'å‹', 'å®ˆå‚™ç‡', 'å¤±ç­–']):
                    headers = row_data
                    header_found = True
                    continue
                elif 'é¸æ‰‹å' in str(row_data) or 'æ‰“å¸­' in str(row_data) or 'è©¦åˆ' in str(row_data):
                    headers = row_data
                    header_found = True
                    continue
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç†ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã£ãŸå¾Œï¼‰
        if header_found and len(row_data) >= 5:
            # é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ã¨æ€ã‚ã‚Œã‚‹è¡Œã‚’è¿½åŠ ï¼ˆç©ºè¡Œã‚„åŒºåˆ‡ã‚Šè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            if row_data[0] and not all(cell == '' or cell == '-' for cell in row_data):
                rows.append(row_data)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½¿ç”¨
    if not headers:
        if stats_type == 'bat':
            headers = ['é¸æ‰‹å', 'è©¦åˆ', 'æ‰“å¸­', 'æ‰“æ•°', 'å¾—ç‚¹', 'å®‰æ‰“', 'äºŒå¡æ‰“', 'ä¸‰å¡æ‰“', 'æœ¬å¡æ‰“', 
                      'å¡æ‰“', 'æ‰“ç‚¹', 'ç›—å¡', 'ç›—å¡æ­»', 'çŠ æ‰“', 'çŠ é£›', 'å››çƒ', 'æ•¬é ', 'æ­»çƒ', 
                      'ä¸‰æŒ¯', 'ä½µæ®ºæ‰“', 'æ‰“ç‡', 'é•·æ‰“ç‡', 'å‡ºå¡ç‡']
        elif stats_type == 'pit':
            headers = ['é¸æ‰‹å', 'è©¦åˆ', 'å‹', 'æ•—', 'ã‚»ãƒ¼ãƒ–', 'ãƒ›ãƒ¼ãƒ«ãƒ‰', 'å®ŒæŠ•', 'å®Œå°', 'ç„¡å››çƒ', 
                      'æŠ•çƒå›', 'æ‰“è€…', 'è¢«å®‰æ‰“', 'è¢«æœ¬å¡æ‰“', 'ä¸å››çƒ', 'æ•¬é ', 'ä¸æ­»çƒ', 'å¥ªä¸‰æŒ¯', 
                      'æš´æŠ•', 'ãƒœãƒ¼ã‚¯', 'å¤±ç‚¹', 'è‡ªè²¬ç‚¹', 'é˜²å¾¡ç‡', 'WHIP', 'è¢«æ‰“ç‡']
        else:  # fld
            headers = ['é¸æ‰‹å', 'è©¦åˆ', 'ã‚¤ãƒ‹ãƒ³ã‚°', 'å®ˆå‚™æ©Ÿä¼š', 'åˆºæ®º', 'è£œæ®º', 'å¤±ç­–', 'ä½µæ®º', 
                      'å®ˆå‚™ç‡', 'RF', 'PB', 'SB', 'CS', 'ç›—å¡é˜»æ­¢ç‡']
    
    if not rows:
        print(f"âš ï¸ {team_name}ã®{STATS_TYPES[stats_type]}ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
    # è¡Œã®é•·ã•ã‚’èª¿æ•´ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã¨åˆã‚ã›ã‚‹ï¼‰
    max_cols = max(len(headers), max(len(row) for row in rows) if rows else 0)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®é•·ã•ã‚’èª¿æ•´
    while len(headers) < max_cols:
        headers.append(f'åˆ—{len(headers)+1}')
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã®é•·ã•ã‚’èª¿æ•´
    adjusted_rows = []
    for row in rows:
        while len(row) < len(headers):
            row.append('')
        adjusted_rows.append(row[:len(headers)])  # ä½™åˆ†ãªåˆ—ã‚’å‰Šé™¤
    
    if not adjusted_rows:
        print(f"âš ï¸ {team_name}ã®{STATS_TYPES[stats_type]}ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    
    # DataFrameã‚’ä½œæˆ
    df = pd.DataFrame(adjusted_rows, columns=headers)
    df['çƒå›£'] = team_name
    df['æˆç¸¾ç¨®åˆ¥'] = STATS_TYPES[stats_type]
    
    return df

def scrape_all_teams():
    """å…¨12çƒå›£ã®æˆç¸¾ã‚’å–å¾—"""
    all_data = {
        'batting': [],
        'pitching': [],
        'fielding': []
    }
    
    print("ğŸš€ NPB 2025å¹´åº¦çƒå›£åˆ¥å€‹äººæˆç¸¾ã®å–å¾—ã‚’é–‹å§‹ã™ã‚‹ã‚ˆã€œ")
    print(f"ğŸ“Š å¯¾è±¡: {len(TEAMS)}çƒå›£ Ã— {len(STATS_TYPES)}ç¨®é¡ = {len(TEAMS) * len(STATS_TYPES)}ãƒšãƒ¼ã‚¸")
    
    for team_code, team_name in TEAMS.items():
        print(f"\nâš¾ {team_name}ã®æˆç¸¾ã‚’å–å¾—ä¸­...")
        
        for stats_code, stats_name in STATS_TYPES.items():
            # URLã‚’æ§‹ç¯‰ï¼ˆä¾‹: idb1_g.html for å·¨äººã®æ‰“æ’ƒæˆç¸¾ï¼‰
            if stats_code == 'bat':
                url_pattern = f'idb1_{team_code}.html'
            elif stats_code == 'pit':
                url_pattern = f'idp1_{team_code}.html'
            else:  # fld
                url_pattern = f'idf1_{team_code}.html'
            
            url = urljoin(BASE_URL, url_pattern)
            print(f"  ğŸ“ˆ {stats_name}: {url}")
            
            # ãƒšãƒ¼ã‚¸å–å¾—
            html_content = get_page_content(url)
            if html_content:
                # ãƒ‡ãƒ¼ã‚¿è§£æ
                df = parse_stats_table(html_content, team_name, stats_code)
                if df is not None and not df.empty:
                    if stats_code == 'bat':
                        all_data['batting'].append(df)
                    elif stats_code == 'pit':
                        all_data['pitching'].append(df)
                    else:  # fld
                        all_data['fielding'].append(df)
                    print(f"    âœ… {len(df)}é¸æ‰‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
                else:
                    print(f"    âŒ ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—")
            
            # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†å¾…æ©Ÿ
            time.sleep(1)
    
    return all_data

def save_to_csv(all_data):
    """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    output_dir = "c:\\Users\\tkeyd\\git\\BestPlayBaseBall\\result"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ’¾ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜é–‹å§‹...")
    
    for stats_type, data_list in all_data.items():
        if data_list:
            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            combined_df = pd.concat(data_list, ignore_index=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
            filename = f"npb_2025_{stats_type}_stats.csv"
            filepath = os.path.join(output_dir, filename)
            
            # CSVä¿å­˜
            combined_df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            print(f"  ğŸ“ {filename}: {len(combined_df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
            print(f"    ãƒ‘ã‚¹: {filepath}")
        else:
            print(f"  âš ï¸ {stats_type}ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ NPB 2025å¹´åº¦çƒå›£åˆ¥å€‹äººæˆç¸¾å–å¾—ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    # å…¨çƒå›£ã®æˆç¸¾ã‚’å–å¾—
    all_data = scrape_all_teams()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    save_to_csv(all_data)
    
    print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("ğŸ“Š å–å¾—ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’resultãƒ•ã‚©ãƒ«ãƒ€ã§ç¢ºèªã—ã¦ã­ã€œ")

if __name__ == "__main__":
    main()
